# -*- coding: utf-8 -*-
"""Movie_Recommendation_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10IrZwRT1gfhzC9l43_PKuB6Q8ZmP0C4H

# Proyek System Rekomendasi : Movie Recommendation

- Nama: Usamah Putra Firdaus
- Email: usamahfirdaa@gmail.com
- ID Dicoding: Usamah Putra Firdaus
"""

!pip install kaggle

"""# **Import Library**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""# **Data Load**"""

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d nicoletacilibiu/movies-and-ratings-for-recommendation-system
!unzip movies-and-ratings-for-recommendation-system.zip

movies_df = pd.read_csv('movies.csv')
movies_df.head()

ratings_df = pd.read_csv('ratings.csv')
ratings_df.head()

"""# **Data Understanding**

## **Data Basic Information**
"""

movies_df.info()

movies_df.shape

ratings_df.info()

ratings_df.shape

n_ratings = len(ratings_df)
n_movies = len(ratings_df['movieId'].unique())
n_users = len(ratings_df['userId'].unique())

print(f"Jumlah Data Rating: {n_ratings}")
print(f"Jumlah Data Movie: {n_movies}")
print(f"Jumlah Data User: {n_users}")
print(f"Rata-rata Rating per User: {round(n_ratings/n_users, 2)}")
print(f"Rara-rata Rating per Movie: {round(n_ratings/n_movies,2)}")

"""# **Data Preparation**

## **Data Cleaning**

### **Movies**
"""

movies_df

"""Memisahkan tahun dalam judul film, dan membuat kolom baru `year_of_release`"""

movies_df['year_of_release'] = movies_df.title.str.extract('(\(\d\d\d\d\))', expand=False)
movies_df.head()

"""kode diatas memisahkan tahun dalam judul. Akan dicari tahun (4 digit angka) yang berada di dalam tanda kurung ( ). Tujuannya agar tidak salah menangkap angka lain di dalam judul (misalnya angka dalam "2001: A Space Odyssey")."""

movies_df['year_of_release'] = movies_df.year_of_release.str.extract('(\d\d\d\d)',expand=False)
movies_df.head(3)

movies_df['title'] = movies_df['title'].str.replace('(\(\d\d\d\d\))', '', regex=True)
movies_df.head()

"""### **Ratings**"""

ratings_df

# Dropping the timestamp column
ratings_df.drop('timestamp', axis=1, inplace=True)

# Confirming the drop
ratings_df.head(3)

"""Karena pada proyek ini tidak memerlukan timestamp, jadi kolom tersebut dihapus

## **Data Merging**
"""

# merge dataframe
films = pd.merge(movies_df, ratings_df, on='movieId', how='left')
films

"""### Data Cleaning"""

# check missing values
(films.isnull() | films.empty | films.isna()).sum()

"""Karena hanya sedikit yang terdapat nilai kosong, sehingga dihapus saja"""

# handling missing values
films = films.dropna()
films

# recheck missing values
(films.isnull() | films.empty | films.isna()).sum()

all_genres = set()
films['genres'].str.split('|').apply(all_genres.update)

print('Jumlah genre unik:', len(all_genres))
print('Daftar genre unik:', all_genres)

"""setelah dilihat genres yang unik, terdapat genre yang aneh seperti `listed)`, `(no`, dan `genres`, sebenarnya genre tersebut adalah satu kesatuan, yaitu `(no genres listed)`, selanjutnya akan dilakukan breakdown terlebih dahulu"""

# Menampilkan film dengan genre '(no genres listed)'
no_genre_movies = films[films['genres'] == '(no genres listed)']

# Menampilkan hasil
print('Jumlah film dengan genre (no genres listed):', len(no_genre_movies))
no_genre_movies

"""Terdapat 37 film yang tidak memiliki genre, jadi film yang tidak memiliki genre akan dihapus karena tidak digunakan dalam tahapa modeling"""

films = films[films['genres'] != '(no genres listed)']

all_genres = set()
films['genres'].str.split('|').apply(all_genres.update)

print('Jumlah genre unik:', len(all_genres))
print('Daftar genre unik:', all_genres)

films.loc[:, 'genres'] = films['genres'].str.lower().str.replace('|', ' ', regex=False)

"""Selanjutnya, mari kita pisahkan nilai-nilai pada kolom genres menjadi `(' ')` agar lebih mudah untuk dibaca

### **Membuat 2 Variable Dataframe Berbeda**

Karena tujuan kita membuat modeling dengan 2 metode yang berbeda yaitu `Content Based Filtering` dan `Collaborative Filtering`. Maka treatment setiap dataset akan berbeda
"""

content_df = films.copy()
content_df

collab_df = films.copy()
collab_df

"""### Handling Data untuk Content Based Filtering

Karena datafrane digunakan untuk metode Content Based Filtering sehingga nama film yang sama perlu dihapus terlebih dahulu
"""

# duplicated by movieId
content_df.duplicated('movieId').sum()

# duplicated by title
content_df.duplicated('title').sum()

# drop duplicated data by movieId & title
content_df = films.drop_duplicates('movieId')
content_df = films.drop_duplicates('title')

content_df

# Bersihkan spasi di kolom
content_df.loc[:, 'title'] = content_df['title'].str.strip()
content_df.loc[:, 'genres'] = content_df['genres'].str.strip()

content_df[content_df.title.eq('Toy Story')]

"""### Handling Data untuk Collaborative Filtering"""

collab_df.loc[:, 'title'] = collab_df['title'].str.strip()

collab_df = collab_df.pivot_table(index="userId", columns="title", values="rating")
collab_df

"""Karena dilihat dari Exploratory Data, setiap film hanya menerima rata-rata sekitar 10 rating dari user, jadi kita asumsikan banyak film yang hanya di rating 1-5 oleh user yang membuat rata-rata rating cukup rendah, sehingga kita abaikan saja film-film yang dirating <5 user. Serta dilihat banyak nilai NaN pada hasil diatas, dimana maksud NaN adalah user tersebut belum merating film tersebut, sehingga kita isi dengan nilai 0 untuk merepresentasikan film tersebut belum dirating oleh user tersebut"""

collab_df = collab_df.dropna(thresh=5, axis=1).fillna(0)
collab_df

"""# **Modeling**

## **Content Based Filtering**
"""

data = content_df
data.sample(5)

"""### **TF-IDF Vektorisasi**"""

# Misal films sudah ada
data = content_df.copy()

# Lanjutkan proses TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(token_pattern=r"(?u)\b[\w-]+\b")
tfidf_matrix = tfidf.fit_transform(data['genres'])

print("Fitur TF-IDF:", tfidf.get_feature_names_out())

tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())
print(tfidf_df.head())

tfidf_matrix = tfidf.fit_transform(data['genres'])

tfidf_matrix.shape

# change tf-idf vector to matrix form
tfidf_matrix.todense()

df_tfidf = pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tfidf.get_feature_names_out(),
    index=data.title
)

# Cek ukuran baris dan kolom
num_rows, num_cols = df_tfidf.shape

# Ambil sampel yang aman
sampled_df = df_tfidf.sample(min(10, num_rows), axis=0).sample(min(20, num_cols), axis=1)

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tfidf.get_feature_names_out(),
    index=data.title
).sample(19, axis=1).sample(10, axis=0)

"""### **Cosine Similarity**"""

# calculate cosine similarity on matrix
from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

# create dataframe from the results of cosine similarity
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['title'], columns=data['title'])
print('Shape:', cosine_sim_df.shape)

# show similarity matrix
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""### **Get Recommendation**"""

# function recommendations
def film_recommendations(title, similarity_data=cosine_sim_df, items=data[['title', 'genres']], k=5):
    # get data index
    index = similarity_data.loc[:,title].to_numpy().argpartition(
        range(-1, -k, -1))

    # retrieve data from an existing index
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # drop title you want to search
    closest = closest.drop(title, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

# sample data
data.sample(3)

data[data.title.eq('Saving Christmas')]

film_recommendations('Saving Christmas')

"""### Evaluation"""

def evaluate_recommendations(title, k=5):
    # Ambil rekomendasi
    recommended = film_recommendations(title, k=k)

    # Ambil genre dari film utama
    main_genres = set(data.loc[data['title'] == title, 'genres'].iloc[0].split(', '))

    results = []
    for rec_title in recommended['title']:
        rec_genres = set(data.loc[data['title'] == rec_title, 'genres'].iloc[0].split(', '))
        common_genres = main_genres.intersection(rec_genres)
        similarity_score = len(common_genres) / len(main_genres) if main_genres else 0
        results.append({
            'Recommended Title': rec_title,
            'Common Genres': common_genres,
            'Genre Similarity': round(similarity_score, 2)
        })

    return pd.DataFrame(results)

evaluate_recommendations('Saving Christmas', k=5)

"""## **Collaborative Filtering**"""

from scipy import sparse
from sklearn.metrics.pairwise import cosine_similarity

"""### **Cosine Similarity**"""

def standardize(row):
    return (row - row.mean()) / (row.max() - row.min())

data_std = collab_df.apply(standardize)
item_similarity = cosine_similarity(data_std.T)
item_similarity = pd.DataFrame(item_similarity, index=collab_df.columns, columns=collab_df.columns)
item_similarity

def get_similar_movies(movie_name, user_rating):
    if movie_name not in item_similarity:
        print(f"Not Found: {movie_name}")
        return
    similar_score = item_similarity[movie_name] * (user_rating-2.5)  # manipulate the similar score using given rating by user
    similar_score = pd.DataFrame(similar_score).T
    return np.array(similar_score).reshape(-1)

print(get_similar_movies("Zodiac", 3.5))

"""### **Membuat User Profile**"""

# ambil 10 judul film acak tanpa menampilkan userId/rating
sample_titles = collab_df.columns.to_series().sample(10).tolist()

sample_titles

usamah_profile = [
    ("Zodiac", 3.5),
    ("Muse, The", 2.0),
    ("Rambo: First Blood Part II", 2.5),
    ("Doc Hollywood", 4.5),
    ("Erin Brockovich", 5.0),
    ("Fantasia", 3.0),
    ("Fun with Dick and Jane", 4.5),
    ("Brothers", 5.0),
    ("Indecent Proposal", 4.0)
]

"""### **Mendapatkan Rekomendasi Berdasarkan User Profile**"""

similar_movies = pd.DataFrame(columns=collab_df.columns)

for i, (movie, rating) in enumerate(usamah_profile):
    sim_scores = get_similar_movies(movie, rating)
    similar_movies.loc[i] = sim_scores

# Profil user (film yang sudah ditonton)
watched_movies = [movie for movie, rating in usamah_profile]
recommendation_scores = similar_movies.sum().sort_values(ascending=False)
recommendation_scores = recommendation_scores.drop(labels=watched_movies, errors='ignore')

print(recommendation_scores[:10])

"""### Evaluation"""

sample_titles = collab_df.columns.to_series().sample(10).tolist()

sample_titles

top_recommendations = recommendation_scores[:10].index.tolist()

# Buat penilaian manual untuk evaluasi (1 = tidak relevan, 5 = sangat relevan)
manual_relevance = {
    "Frequency": 5,
    "Meatballs": 4,
    "Unstoppable": 4,
    "Night on Earth": 2,
    "Dragonslayer": 5,
    "Fly, The": 3,
    "Walk the Line": 2,
    "Chasing Liberty": 5,
    "Pee-wee's Big Adventure": 4,
    "Shrek 2": 3,
}

# Ambil hanya film yang dinilai secara manual
evaluated_scores = recommendation_scores[manual_relevance.keys()]

# Buat DataFrame evaluasi
evaluation_df = pd.DataFrame({
    'Predicted Score': evaluated_scores,
    'Manual Relevance': pd.Series(manual_relevance)
})

# Normalisasi skor prediksi
evaluation_df['Normalized Predicted'] = evaluation_df['Predicted Score'] / evaluation_df['Predicted Score'].max()

# Hitung korelasi antara skor sistem dan penilaian manual
correlation = evaluation_df[['Normalized Predicted', 'Manual Relevance']].corr().iloc[0, 1]

# Tampilkan hasil evaluasi
print("=== Evaluation Result ===")
print(evaluation_df)
print(f"\nCorrelation between predicted and manual relevance: {correlation:.2f}")